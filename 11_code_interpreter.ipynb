{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf108b94",
   "metadata": {},
   "source": [
    "# Code Interpreter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e896f48",
   "metadata": {},
   "source": [
    "Traditional LLMs are good at generating text, but they struggle with tasks that require math or calculations.\n",
    "\n",
    "**Example:** How many \"r\"s are present in the string \"strawberry\"?  \n",
    "**Answer from LLM:** \"strawberry\" has 2 \"r\"s.\n",
    "\n",
    "**Yikes!**\n",
    "\n",
    "Discussions regarding LLMs can't count:\n",
    "- [Should a custom GPT be able to count the number of items in a JSON list?](https://community.openai.com/t/should-a-custom-gpt-be-able-to-count-the-number-of-items-in-a-json-list/575999)\n",
    "- [Assistant can not search the whole file using file search](https://community.openai.com/t/assistant-can-not-search-the-whole-file-using-file-search/739661/3)\n",
    "- [How do I pass complex and nested large JSON data](https://www.reddit.com/r/OpenAI/comments/15xfcuk/how_do_i_pass_complex_and_nested_large_json_data)\n",
    "\n",
    "To solve this problem, OpenAI has introduced a feature called **\"Code Interpreter\"**.\n",
    "\n",
    "With Code Interpreter enabled:\n",
    "- The LLM will generate **Python code** to solve the problem.\n",
    "- The code is executed in a container.\n",
    "- If the code fails, the LLM automatically debugs and refines it until it executes successfully.\n",
    "- Based on the code execution results, the LLM generates a final answer.\n",
    "\n",
    "By writing Python code, your LLM can solve code, math, and data analysis problems now.\n",
    "\n",
    "### Additional cost of using Code Interpreter  \n",
    "Code Interpreter has additional charges beyond the token based fees for Azure OpenAI usage. Check: [Azure OpenAI Service Pricing](https://azure.microsoft.com/en-gb/pricing/details/cognitive-services/openai-service/)\n",
    "\n",
    "### References\n",
    "\n",
    "- [OpenAI Code Interpreter Documentation](https://platform.openai.com/docs/assistants/tools/code-interpreter)\n",
    "- [Azure AI Foundry - Responses with Code Interpreter](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/responses?tabs=python-key#code-interpreter)\n",
    "- [Azure OpenAI Code Interpreter How-to](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/code-interpreter?tabs=python)\n",
    "- [OpenAI Assistants Quickstart](https://platform.openai.com/docs/assistants/quickstart?example=without-streaming)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113f112d",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. Make sure that `python3` is installed on your system.\n",
    "1. Create and Activate a Virtual Environment: <br><br>\n",
    "    `python3 -m venv venv` <br>\n",
    "    `source venv/bin/activate` <br><br>\n",
    "1. Create a `.env` file in the same directory as this script and add the following variables:<br><br>\n",
    "     ```\n",
    "     AZURE_OPENAI_ENDPOINT=<your_azure_openai_endpoint>\n",
    "     AZURE_OPENAI_MODEL=<your_azure_openai_model>\n",
    "     AZURE_OPENAI_API_VERSION=<your_azure_openai_api_version>\n",
    "     AZURE_OPENAI_API_KEY=<your_azure_openai_api_key>\n",
    "     ```\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c4a82c",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "\n",
    "The required libraries are listed in the requirements.txt file. Use the following command to install them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "409191ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: tiktoken in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 2)) (0.9.0)\n",
      "Requirement already satisfied: openai in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 3)) (1.78.0)\n",
      "Requirement already satisfied: dotenv in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 4)) (0.9.9)\n",
      "Requirement already satisfied: pydantic in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 5)) (2.11.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.13/site-packages (from requests->-r requirements.txt (line 1)) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.13/site-packages (from requests->-r requirements.txt (line 1)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.13/site-packages (from requests->-r requirements.txt (line 1)) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.13/site-packages (from requests->-r requirements.txt (line 1)) (2025.4.26)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./venv/lib/python3.13/site-packages (from tiktoken->-r requirements.txt (line 2)) (2024.11.6)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./venv/lib/python3.13/site-packages (from openai->-r requirements.txt (line 3)) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./venv/lib/python3.13/site-packages (from openai->-r requirements.txt (line 3)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./venv/lib/python3.13/site-packages (from openai->-r requirements.txt (line 3)) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./venv/lib/python3.13/site-packages (from openai->-r requirements.txt (line 3)) (0.9.0)\n",
      "Requirement already satisfied: sniffio in ./venv/lib/python3.13/site-packages (from openai->-r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./venv/lib/python3.13/site-packages (from openai->-r requirements.txt (line 3)) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in ./venv/lib/python3.13/site-packages (from openai->-r requirements.txt (line 3)) (4.13.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.13/site-packages (from pydantic->-r requirements.txt (line 5)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./venv/lib/python3.13/site-packages (from pydantic->-r requirements.txt (line 5)) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./venv/lib/python3.13/site-packages (from pydantic->-r requirements.txt (line 5)) (0.4.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 3)) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 3)) (0.16.0)\n",
      "Requirement already satisfied: python-dotenv in ./venv/lib/python3.13/site-packages (from dotenv->-r requirements.txt (line 4)) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c816d0f",
   "metadata": {},
   "source": [
    "***\n",
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e904c82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI  # The `AzureOpenAI` library is used to interact with the Azure OpenAI API.\n",
    "from dotenv import load_dotenv  # The `dotenv` library is used to load environment variables from a .env file.\n",
    "import os                       # Used to get the values from environment variables.\n",
    "from pprint import pprint       # The `pprint` library is used to pretty-print a dictionary\n",
    "import json                     # The `json` library is used to work with JSON data in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cfffcb",
   "metadata": {},
   "source": [
    "## Load environment variables from .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "164f5cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1976d37f",
   "metadata": {},
   "source": [
    "## Initialize the Azure OpenAI Client\n",
    "\n",
    "We extract the environment variables and store them explicitly to ensure they're available, then initialize the client using these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f498eeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract environment variables and store them explicitly to ensure they're available\n",
    "AZURE_OPENAI_ENDPOINT        = os.environ['AZURE_OPENAI_ENDPOINT']\n",
    "AZURE_OPENAI_MODEL           = os.environ['AZURE_OPENAI_MODEL']\n",
    "AZURE_OPENAI_API_VERSION     = os.environ['AZURE_OPENAI_VERSION']\n",
    "AZURE_OPENAI_API_KEY         = os.environ['AZURE_OPENAI_API_KEY']\n",
    "\n",
    "# Initialize the client using the extracted variables\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint = AZURE_OPENAI_ENDPOINT,\n",
    "    api_key = AZURE_OPENAI_API_KEY,  \n",
    "    api_version = AZURE_OPENAI_API_VERSION\n",
    ")\n",
    "\n",
    "deployment_name = AZURE_OPENAI_MODEL  # The deployment name of the model to use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7304734a",
   "metadata": {},
   "source": [
    "## Upload your file to Azure Server with an \"assistants\" purpose\n",
    "\n",
    "What is purpose?\n",
    "- When you upload a file to Azure OpenAI, you need to specify the purpose of the file.\n",
    "- The following purposes are supported: https://learn.microsoft.com/en-us/rest/api/azureopenai/files/upload?view=rest-azureopenai-2024-10-21&tabs=HTTP#purpose\n",
    "\n",
    "What file formats are supported for upload?<br>\n",
    "https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/code-interpreter?tabs=python#supported-file-types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21c9e98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file, file ID: assistant-2LyGyJxefoq33SnXNKeySk\n"
     ]
    }
   ],
   "source": [
    "file = client.files.create(\n",
    "    file=open(\"dummy_build_data.json\", \"rb\"), #multipart file upload requires the file to be in binary not in text\n",
    "    purpose='assistants' # This file contains data to be used by AI assistants.\n",
    ")\n",
    "\n",
    "# Use file.id to refer to the file\n",
    "print(f\"Uploaded file, file ID: {file.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a53c1f",
   "metadata": {},
   "source": [
    "Note: You cannot view the content of a file uploaded to the Azure OpenAI server if the purpose is defined as `assistants`\n",
    "\n",
    "The following code will not work:\n",
    "```\n",
    "uploaded_file_content = client.files.content(file.id)\n",
    "```\n",
    "\n",
    "The above command will throw the following error:\n",
    "```\n",
    "openai.error.InvalidRequestError: The file content is not available for the purpose of \"assistants\".\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f4567f",
   "metadata": {},
   "source": [
    "## Send your request to the Azure OpenAI API, this time with Code Interpreter enabled\n",
    "\n",
    "Note two additional parameters:\n",
    "- `tools=[{\"type\": \"code_interpreter\"}]`: Empower with Code Interpreter capabilities\n",
    "- `stream=True`: Enables streaming responses, allowing us to see the code execution process in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a4ef00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.create(\n",
    "    model = AZURE_OPENAI_MODEL,\n",
    "    instructions = f\"\"\"\n",
    "        # Instructions\n",
    "        - The JSON file contains Jenkins build information under the key `results`\n",
    "        - Each entry in the `results` array contains information about a build.\n",
    "        - Build status of a build can be found by checking the `build_status` key.\n",
    "        - Build duration (time build took to complete) can be found by checking the `build_duration` key.\n",
    "        - Queue time (time build spent in queue) can be found by checking the `queue_time` key.\n",
    "        - Build label can be found by checking the `build_label` key. When somebody ask about a build, make sure to provide the build label.\n",
    "        \"\"\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Provide Total builds and list all build statuses along their counts and percentages. \"\n",
    "                        \"Also provide the fastest and the slowest build along with their build duration. \"\n",
    "                        \"Also provide the build labels with the longest and shortest queue time. Provide durations too. \"\n",
    "                        \"Also provide the average build and queue duration. \"\n",
    "        }\n",
    "    ],\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"code_interpreter\", # Use code interpreter\n",
    "            \"container\": {              # Spin up a container for the LLM to run Python code\n",
    "                \"type\": \"auto\",         # Let Azure OpenAI decide the best container type to create. The container will auto-expire if not used for 20 minutes.\n",
    "                \"file_ids\": [file.id]   # Add the uploaded file to the container so that LLM can access it\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    stream=True     # Its wise to enable streaming for code_interpreter to let users see what's happening behind the scenes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2d44ab",
   "metadata": {},
   "source": [
    "## Print the chunks as they come in\n",
    "\n",
    "The incoming chunks will also contain LLM's internal monologues related to code generation and interpretation.\n",
    "\n",
    "Apart from the usual chunk types, when code_interpreter is used, you may also see:\n",
    "- `response.code_interpreter_call_code.delta`: LLM is generating code\n",
    "- `response.code_interpreter_call_code.done`: LLM has finished generating code\n",
    "- `response.code_interpreter_call.interpreting`: LLM code is being interpreted\n",
    "- `response.code_interpreter_call.completed`: LLM code interpretation is complete\n",
    "\n",
    "**API Reference:** [Response Streaming with Code Interpreter](https://platform.openai.com/docs/api-reference/responses-streaming/response/code_interpreter_call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7008ec46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "AI Analysis Started\n",
      "--------------------------------------------------------------------------------\n",
      "I will start by loading the JSON file to inspect its structure and then proceed to analyze the Jenkins build information based on your requirements.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "import json\n",
      "\n",
      "# Load the JSON file\n",
      "file_path = '/mnt/data/assistant-2LyGyJxefoq33SnXNKeySk-dummy_build_data.json'\n",
      "with open(file_path, 'r') as file:\n",
      "    data = json.load(file)\n",
      "\n",
      "# Inspect the first few entries in the 'results' to understand the structure\n",
      "data['results'][:3]\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Code generation complete.\n",
      "Code is being interpreted...\n",
      "Code interpretation complete ...\n",
      "--------------------------------------------------------------------------------\n",
      "The JSON contains an array of build dictionaries under the \"results\" key. Each build dictionary contains the keys `build_status`, `build_duration`, `queue_time`, and `build_label` among others.\n",
      "\n",
      "I will now process these builds to provide\n",
      "- Total builds\n",
      "- List of all build statuses with their counts and percentages\n",
      "- The fastest and slowest build with their durations\n",
      "- The build labels with the longest and shortest queue times with durations\n",
      "- The average build duration and average queue time\n",
      "\n",
      "I will start by converting the duration times (build and queue) from string format to seconds for easier comparison and calculation, then proceed with the analysis.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "from datetime import datetime, timedelta\n",
      "from collections import Counter\n",
      "\n",
      "# Helper function to convert HH:MM:SS.sss format to seconds\n",
      "def duration_to_seconds(duration):\n",
      "    if '.' in duration:\n",
      "        time_part, micro_part = duration.split('.')\n",
      "        microseconds = int(micro_part.ljust(6, '0'))  # add trailing zeros to microsecond precision\n",
      "    else:\n",
      "        time_part = duration\n",
      "        microseconds = 0\n",
      "    dt = datetime.strptime(time_part, \"%H:%M:%S\")\n",
      "    td = timedelta(hours=dt.hour, minutes=dt.minute, seconds=dt.second, microseconds=microseconds)\n",
      "    return td.total_seconds()\n",
      "\n",
      "builds = data['results']\n",
      "\n",
      "# Extract needed info and convert durations to seconds\n",
      "build_duration_secs = []\n",
      "queue_time_secs = []\n",
      "build_statuses = []\n",
      "build_labels = []\n",
      "\n",
      "for build in builds:\n",
      "    build_duration_secs.append(duration_to_seconds(build['build_duration']))\n",
      "    queue_time_secs.append(duration_to_seconds(build['queue_time']))\n",
      "    build_statuses.append(build['build_status'])\n",
      "    build_labels.append(build['build_label'])\n",
      "\n",
      "# Total builds\n",
      "total_builds = len(builds)\n",
      "\n",
      "# Build status counts and percentages\n",
      "status_counts = Counter(build_statuses)\n",
      "status_percentages = {status: (count / total_builds) * 100 for status, count in status_counts.items()}\n",
      "\n",
      "# Fastest and slowest build\n",
      "fastest_build_index = build_duration_secs.index(min(build_duration_secs))\n",
      "slowest_build_index = build_duration_secs.index(max(build_duration_secs))\n",
      "fastest_build = {\n",
      "    'build_label': build_labels[fastest_build_index],\n",
      "    'build_duration': builds[fastest_build_index]['build_duration']\n",
      "}\n",
      "slowest_build = {\n",
      "    'build_label': build_labels[slowest_build_index],\n",
      "    'build_duration': builds[slowest_build_index]['build_duration']\n",
      "}\n",
      "\n",
      "# Build labels with longest and shortest queue time\n",
      "shortest_queue_index = queue_time_secs.index(min(queue_time_secs))\n",
      "longest_queue_index = queue_time_secs.index(max(queue_time_secs))\n",
      "shortest_queue_build = {\n",
      "    'build_label': build_labels[shortest_queue_index],\n",
      "    'queue_time': builds[shortest_queue_index]['queue_time']\n",
      "}\n",
      "longest_queue_build = {\n",
      "    'build_label': build_labels[longest_queue_index],\n",
      "    'queue_time': builds[longest_queue_index]['queue_time']\n",
      "}\n",
      "\n",
      "# Average build and queue duration\n",
      "average_build_duration_sec = sum(build_duration_secs) / total_builds\n",
      "average_queue_time_sec = sum(queue_time_secs) / total_builds\n",
      "\n",
      "average_build_duration = str(timedelta(seconds=average_build_duration_sec))\n",
      "average_queue_time = str(timedelta(seconds=average_queue_time_sec))\n",
      "\n",
      "{\n",
      "    'total_builds': total_builds,\n",
      "    'status_counts': status_counts,\n",
      "    'status_percentages': status_percentages,\n",
      "    'fastest_build': fastest_build,\n",
      "    'slowest_build': slowest_build,\n",
      "    'shortest_queue_build': shortest_queue_build,\n",
      "    'longest_queue_build': longest_queue_build,\n",
      "    'average_build_duration': average_build_duration,\n",
      "    'average_queue_time': average_queue_time\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Code generation complete.\n",
      "Code is being interpreted...\n",
      "Code interpretation complete ...\n",
      "--------------------------------------------------------------------------------\n",
      "Here is the Jenkins build information summary based on the provided data:\n",
      "\n",
      "- Total builds: 33\n",
      "\n",
      "- Build statuses with counts and percentages:\n",
      "  - SUCCESS: 24 (72.73%)\n",
      "  - FAILURE: 5 (15.15%)\n",
      "  - ABORTED: 3 (9.09%)\n",
      "  - UNSTABLE: 1 (3.03%)\n",
      "\n",
      "- Fastest build:\n",
      "  - Build label: XYZ-v1_2_0-BUILD_1434\n",
      "  - Build duration: 00:21:12.56\n",
      "\n",
      "- Slowest build:\n",
      "  - Build label: XYZ-v1_2_0-BUILD_1440\n",
      "  - Build duration: 11:43:01.094\n",
      "\n",
      "- Build with shortest queue time:\n",
      "  - Build label: XYZ-v1_2_0-BUILD_1435\n",
      "  - Queue time: 00:00:00.003\n",
      "\n",
      "- Build with longest queue time:\n",
      "  - Build label: XYZ-v1_2_0-BUILD_1434\n",
      "  - Queue time: 00:30:03.624\n",
      "\n",
      "- Average build duration: 3:47:18.58 (hours:minutes:seconds)\n",
      "- Average queue time: 0:01:45.43 (hours:minutes:seconds)\n",
      "\n",
      "If you need any more detailed analysis or information, feel free to ask!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Analysis Complete\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for chunk in response:\n",
    "    if chunk.type == 'response.created': # LLM has started responding\n",
    "        print(\"-\" * 80)\n",
    "        print(\"AI Analysis Started\")\n",
    "        print(\"-\" * 80)\n",
    "    elif chunk.type == 'response.code_interpreter_call_code.delta': # LLM is generating code in chunks. Keep printing them as they come in\n",
    "        code = chunk.delta\n",
    "        print(code, end='', flush=True)\n",
    "    elif chunk.type == 'response.code_interpreter_call_code.done': # LLM has finished generating code\n",
    "        print(\"\\n\")\n",
    "        print(\"-\" * 80)\n",
    "        print(\"Code generation complete.\")\n",
    "    elif chunk.type == 'response.code_interpreter_call.interpreting': # LLM code is being interpreted\n",
    "        print(\"Code is being interpreted...\")\n",
    "    elif chunk.type == 'response.code_interpreter_call.completed': # LLM code interpretation is complete\n",
    "        print(\"Code interpretation complete ...\")\n",
    "        print(\"-\" * 80)\n",
    "    elif chunk.type == 'response.output_text.delta': # LLM is responding in chunks. Keep printing them as they come in\n",
    "        partial_llm_response = chunk.delta\n",
    "        print(partial_llm_response, end='', flush=True)\n",
    "    elif chunk.type == 'response.output_text.done': # LLM response is complete\n",
    "        print(\"\\n\")\n",
    "        print(\"-\" * 80)\n",
    "    elif chunk.type == 'response.completed': # LLM has finished responding\n",
    "        print(\"Analysis Complete\")\n",
    "        print(\"-\" * 80)\n",
    "    elif chunk.type == 'response.error': # Error occurred\n",
    "        print(f\"\\nError from LLM: {chunk.error.message}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837dbf5c",
   "metadata": {},
   "source": [
    "## Cleanup - delete the original file from the server to free up space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c588f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted file, file ID: assistant-2LyGyJxefoq33SnXNKeySk\n"
     ]
    }
   ],
   "source": [
    "client.files.delete(file.id)\n",
    "print(f\"Deleted file, file ID: {file.id}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
